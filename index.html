<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <title>AdrianSeeley.com</title>
        <style>
            .container {
                margin: 0 auto;
                max-width: 800px;
                padding: 20px;
            }
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                line-height: 1.6;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Adrian Seeley</h1>
            <h3>Randomized Sparse Connectivity in Deep Neural Networks: A Literature Review and Methodological Framework</h3>
            <div>February 15, 2025</div>
            <p>
                We review the literature on randomized and sparsely connected neural networks and introduce a novel framework in which each neuron in the hidden layers receives a fixed number of random inputs from any preceding layer while the output layer remains fully connected. The proposed methodology is described in detail, including the construction of a directed acyclic graph (DAG) to represent the network, the training procedure on the MNIST dataset, and extensive parameter exploration. Our preliminary results indicate that increasing the number of inputs per neuron yields a higher return on parameter investment compared to increasing layer or neuron counts. Future work will investigate these findings further. The completelete source code CSV results are included as supplementary material.
            </p>
            <ul>
                <li><a href="./Seeley2025-FEB-15-PAPER.pdf">Paper (PDF)</a></li>
                <li><a href="./Seeley2025-FEB-15-RESULTS.csv">Results (CSV)</a></li>
                <li><a href="./Seeley2025-FEB-15-SOURCE.zip">Source Code (ZIP)</a></li>
            </ul>
        </div>
    </body>
</html>
